{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach:\n",
    "\n",
    "- Convert image into 1D array\n",
    "- Save into CSV file\n",
    "- Used the CSV file to Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Problem Definition and Objective Setting\n",
    "\n",
    "    The goal is to classify facial expressions into emotion categories (e.g., anger, happiness, sadness, etc.).\n",
    "    We’ll use accuracy as the primary evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2  # OpenCV for image processing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes (anger, happy, sad, ...)\n",
    "\n",
    "Directory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "emotion_dataset\n",
    "    anger\n",
    "        1.jpg\n",
    "        2.jpg\n",
    "        ...\n",
    "    happy\n",
    "        1.jpg\n",
    "    sad\n",
    "        1.jpg\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Data Collection and Preparation\n",
    "\n",
    "    Download the FER-2013 dataset from Kaggle and unzip it to a directory, for example, fer2013/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach Used: Load the image in a csv. The csv file will used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load Image Dataset into CSV\n",
    "\n",
    "Here’s a complete example using Python, assuming you have a folder of images and you want to create a CSV file with pixel values and labels (if applicable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions:\n",
    "\n",
    "    - Images are in a directory named images/.\n",
    "    - Labels are either embedded in the file names (e.g., happy_01.jpg, sad_02.jpg) or you have a separate list of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion_dataset\n",
    "#     anger_1.jpg\n",
    "#     anger_2.jpg\n",
    "#     happy_1.jpg\n",
    "#     sad_1.jpg\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = 'emotion_image_datasets/'\n",
    "image_size = (48, 48)  # Desired image size (48x48 for FER-2013)\n",
    "\n",
    "# Initialize lists to hold data\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Loop through all images in the directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        \n",
    "        # Load the image using OpenCV\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read image in grayscale\n",
    "        img = cv2.resize(img, image_size)  # Resize image to the desired size\n",
    "\n",
    "        # Flatten the image to a 1D array\n",
    "        img_flat = img.flatten()\n",
    "        \n",
    "        # Extract label from filename (if applicable)\n",
    "        # Example: 'happy_01.jpg' -> label 'happy'\n",
    "        label = filename.split('_')[0]  # Assuming label is the first part before '_'\n",
    "        \n",
    "        # Append image data and label to lists\n",
    "        data.append(img_flat)\n",
    "        labels.append(label)\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "df['emotion'] = labels  # Add the labels as a new column\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_path = 'emotion_datasets.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f'Data saved to {csv_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to emotion_datasets.csv\n"
     ]
    }
   ],
   "source": [
    "# emotion_dataset\n",
    "#     anger\n",
    "#         1.jpg\n",
    "#         2.jpg\n",
    "#         ...\n",
    "#     happy\n",
    "#         1.jpg\n",
    "#     sad\n",
    "#         1.jpg\n",
    "\n",
    "# Directory containing subfolders of images\n",
    "dataset_dir = 'emotion_image_datasets/'\n",
    "image_size = (48, 48)  # Desired image size (e.g., 48x48 for FER-2013)\n",
    "\n",
    "# Initialize lists to hold data\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each folder in the dataset directory\n",
    "for label in os.listdir(dataset_dir):\n",
    "    folder_path = os.path.join(dataset_dir, label)\n",
    "    \n",
    "    # Check if it is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Loop through each image in the subfolder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                \n",
    "                # Load the image using OpenCV\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read image in grayscale\n",
    "                img = cv2.resize(img, image_size)  # Resize image to the desired size\n",
    "\n",
    "                # Flatten the image to a 1D array\n",
    "                img_flat = img.flatten()\n",
    "                \n",
    "                # Convert the flattened image array to a space-separated string\n",
    "                img_flat_str = ' '.join(map(str, img_flat))\n",
    "                \n",
    "                # Append image data and corresponding label to lists\n",
    "                data.append(img_flat_str)  # Use string format for pixel values\n",
    "                labels.append(label)  # Use folder name as label\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame({\n",
    "    'Pixels': data,      # Assign the pixel data\n",
    "    'Emotion': labels    # Assign the labels\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_path = 'emotion_datasets.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f'Data saved to {csv_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the image are loaded into CSV, use the csv file for preparation\n",
    "\n",
    "- load the csv\n",
    "- split the dataset (csv data) into Training, Validation, Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Pixels Emotion\n",
      "0  46 34 26 23 12 32 35 26 26 38 71 99 123 134 14...   angry\n",
      "1  55 33 54 32 26 21 50 29 38 45 79 73 43 22 51 5...   angry\n",
      "2  123 123 126 131 124 69 109 149 159 174 169 178...   angry\n",
      "3  255 255 255 255 255 255 255 255 255 251 254 16...   angry\n",
      "4  127 121 124 137 123 118 120 111 111 109 118 14...   angry\n",
      "['Pixels', 'Emotion']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load data (assuming the data is stored as a CSV with pixel values and labels)\n",
    "data_path = 'emotion_datasets.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Check the structure\n",
    "print(data.head())\n",
    "print(data.columns.tolist()) # Print the list of column names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data arrays for images and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    # Use 'Pixels' with uppercase 'P'\n",
    "    pixels = np.array(row['Pixels'].split(), dtype='float32').reshape(48, 48, 1)\n",
    "    X.append(pixels)\n",
    "    y.append(row['Emotion'])  # Use 'Emotion' with uppercase 'E'\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)  # Integer encode the labels\n",
    "# Now split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, check the data types of your training labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of y_train: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Data type of y_train:\", y_train.dtype)  # Should be an integer type (e.g., int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it’s not, you can convert it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = y_train.astype(np.int32)  # Ensure it's in the correct integer format\n",
    "# y_val = y_val.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Feature Engineering\n",
    "\n",
    "For image data, feature engineering is often handled by convolutional layers in a CNN. However, we can normalize pixel values for better model convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Model Selection\n",
    "\n",
    "A Convolutional Neural Network (CNN) is a suitable choice for image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\2024-2025 SEM 1\\Event-Driven Programming\\Modules\\Part 9 - Machine Learning (ML) Development\\Unsupervised Learning\\Emotion Classification\\mltutzenv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">903</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,048,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m903\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,142,279</span> (4.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,142,279\u001b[0m (4.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,142,279</span> (4.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,142,279\u001b[0m (4.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')  # 7 output neurons for 7 emotion classes\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Model Training\n",
    "\n",
    "Compile and Train the model on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 139ms/step - accuracy: 0.2083 - loss: 1.8739 - val_accuracy: 0.2479 - val_loss: 1.7998\n",
      "Epoch 2/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 132ms/step - accuracy: 0.2376 - loss: 1.8253 - val_accuracy: 0.2683 - val_loss: 1.7610\n",
      "Epoch 3/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 128ms/step - accuracy: 0.2977 - loss: 1.7494 - val_accuracy: 0.3315 - val_loss: 1.6741\n",
      "Epoch 4/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 120ms/step - accuracy: 0.3402 - loss: 1.6822 - val_accuracy: 0.3835 - val_loss: 1.5949\n",
      "Epoch 5/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 124ms/step - accuracy: 0.3976 - loss: 1.5856 - val_accuracy: 0.4076 - val_loss: 1.5519\n",
      "Epoch 6/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 121ms/step - accuracy: 0.4229 - loss: 1.5261 - val_accuracy: 0.4271 - val_loss: 1.5203\n",
      "Epoch 7/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 124ms/step - accuracy: 0.4170 - loss: 1.4798 - val_accuracy: 0.4206 - val_loss: 1.5030\n",
      "Epoch 8/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - accuracy: 0.4591 - loss: 1.4049 - val_accuracy: 0.4299 - val_loss: 1.4608\n",
      "Epoch 9/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 124ms/step - accuracy: 0.4964 - loss: 1.3664 - val_accuracy: 0.4513 - val_loss: 1.4500\n",
      "Epoch 10/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - accuracy: 0.4946 - loss: 1.3091 - val_accuracy: 0.4485 - val_loss: 1.4518\n",
      "Epoch 11/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - accuracy: 0.5259 - loss: 1.2481 - val_accuracy: 0.4485 - val_loss: 1.4222\n",
      "Epoch 12/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - accuracy: 0.5507 - loss: 1.1812 - val_accuracy: 0.4513 - val_loss: 1.4270\n",
      "Epoch 13/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - accuracy: 0.5793 - loss: 1.1044 - val_accuracy: 0.4624 - val_loss: 1.4224\n",
      "Epoch 14/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - accuracy: 0.5959 - loss: 1.0667 - val_accuracy: 0.4513 - val_loss: 1.4436\n",
      "Epoch 15/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.6234 - loss: 0.9741 - val_accuracy: 0.4559 - val_loss: 1.4264\n",
      "Epoch 16/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 121ms/step - accuracy: 0.6468 - loss: 0.9316 - val_accuracy: 0.4652 - val_loss: 1.6230\n",
      "Epoch 17/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - accuracy: 0.6746 - loss: 0.8647 - val_accuracy: 0.4661 - val_loss: 1.5524\n",
      "Epoch 18/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - accuracy: 0.6963 - loss: 0.7965 - val_accuracy: 0.4420 - val_loss: 1.5660\n",
      "Epoch 19/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.7031 - loss: 0.7616 - val_accuracy: 0.4717 - val_loss: 1.6415\n",
      "Epoch 20/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 131ms/step - accuracy: 0.7318 - loss: 0.6933 - val_accuracy: 0.4587 - val_loss: 1.6858\n",
      "Epoch 21/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - accuracy: 0.7533 - loss: 0.6340 - val_accuracy: 0.4670 - val_loss: 1.7736\n",
      "Epoch 22/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - accuracy: 0.7775 - loss: 0.5929 - val_accuracy: 0.4698 - val_loss: 1.8241\n",
      "Epoch 23/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - accuracy: 0.7777 - loss: 0.5671 - val_accuracy: 0.4503 - val_loss: 1.9711\n",
      "Epoch 24/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.8001 - loss: 0.5139 - val_accuracy: 0.4559 - val_loss: 2.1524\n",
      "Epoch 25/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 124ms/step - accuracy: 0.8169 - loss: 0.4570 - val_accuracy: 0.4466 - val_loss: 2.0513\n",
      "Epoch 26/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 119ms/step - accuracy: 0.8179 - loss: 0.4576 - val_accuracy: 0.4680 - val_loss: 2.1984\n",
      "Epoch 27/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.8382 - loss: 0.4162 - val_accuracy: 0.4605 - val_loss: 2.1957\n",
      "Epoch 28/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 121ms/step - accuracy: 0.8541 - loss: 0.3880 - val_accuracy: 0.4652 - val_loss: 2.3099\n",
      "Epoch 29/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - accuracy: 0.8480 - loss: 0.3832 - val_accuracy: 0.4587 - val_loss: 2.4653\n",
      "Epoch 30/30\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 122ms/step - accuracy: 0.8592 - loss: 0.3593 - val_accuracy: 0.4708 - val_loss: 2.4771\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val), batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Model Evaluation\n",
    "\n",
    "Evaluate the model on the validation set to check its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.4828 - loss: 2.3694\n",
      "Validation Accuracy: 0.47\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation data\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Accuracy: {val_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Hyperparameter Tuning\n",
    "\n",
    "You could apply a grid search for tuning, though it’s computationally intensive. Here, we can try adjusting the learning rate, batch size, or network layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Model Testing\n",
    "\n",
    "After final tuning, evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.4354 - loss: 2.6842\n",
      "Test Accuracy: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.22730891406536102,\n",
       "  0.2490047812461853,\n",
       "  0.30254778265953064,\n",
       "  0.3509156107902527,\n",
       "  0.39530253410339355,\n",
       "  0.41600319743156433,\n",
       "  0.41938695311546326,\n",
       "  0.45561304688453674,\n",
       "  0.4852707087993622,\n",
       "  0.506369411945343,\n",
       "  0.5201035141944885,\n",
       "  0.5519506335258484,\n",
       "  0.5696656107902527,\n",
       "  0.5915604829788208,\n",
       "  0.6150477528572083,\n",
       "  0.6453025341033936,\n",
       "  0.6707802414894104,\n",
       "  0.696457028388977,\n",
       "  0.7066082954406738,\n",
       "  0.7366639971733093,\n",
       "  0.7559713125228882,\n",
       "  0.777667224407196,\n",
       "  0.7804538011550903,\n",
       "  0.7991639971733093,\n",
       "  0.8142914175987244,\n",
       "  0.8172770738601685,\n",
       "  0.8240445852279663,\n",
       "  0.847332775592804,\n",
       "  0.8471337556838989,\n",
       "  0.8558917045593262],\n",
       " 'loss': [1.8520427942276,\n",
       "  1.803928256034851,\n",
       "  1.7375102043151855,\n",
       "  1.6598913669586182,\n",
       "  1.5907431840896606,\n",
       "  1.5333325862884521,\n",
       "  1.4815630912780762,\n",
       "  1.4241293668746948,\n",
       "  1.3769943714141846,\n",
       "  1.2992082834243774,\n",
       "  1.2474925518035889,\n",
       "  1.1906825304031372,\n",
       "  1.1313891410827637,\n",
       "  1.0763237476348877,\n",
       "  1.002203345298767,\n",
       "  0.9304331541061401,\n",
       "  0.8683754205703735,\n",
       "  0.8071479201316833,\n",
       "  0.7535335421562195,\n",
       "  0.6953291893005371,\n",
       "  0.6423018574714661,\n",
       "  0.5967701077461243,\n",
       "  0.5709366798400879,\n",
       "  0.5150802731513977,\n",
       "  0.4744914770126343,\n",
       "  0.4584694504737854,\n",
       "  0.44208163022994995,\n",
       "  0.39698415994644165,\n",
       "  0.3864020109176636,\n",
       "  0.36520451307296753],\n",
       " 'val_accuracy': [0.24791085720062256,\n",
       "  0.2683379650115967,\n",
       "  0.3314763307571411,\n",
       "  0.38347262144088745,\n",
       "  0.40761375427246094,\n",
       "  0.427112340927124,\n",
       "  0.42061281204223633,\n",
       "  0.42989787459373474,\n",
       "  0.4512534737586975,\n",
       "  0.4484679698944092,\n",
       "  0.4484679698944092,\n",
       "  0.4512534737586975,\n",
       "  0.4623955488204956,\n",
       "  0.4512534737586975,\n",
       "  0.4558960199356079,\n",
       "  0.46518105268478394,\n",
       "  0.46610957384109497,\n",
       "  0.4419684410095215,\n",
       "  0.47168058156967163,\n",
       "  0.45868152379989624,\n",
       "  0.4670380651950836,\n",
       "  0.46982359886169434,\n",
       "  0.45032498240470886,\n",
       "  0.4558960199356079,\n",
       "  0.4466109573841095,\n",
       "  0.46796658635139465,\n",
       "  0.4605385363101959,\n",
       "  0.46518105268478394,\n",
       "  0.45868152379989624,\n",
       "  0.470752090215683],\n",
       " 'val_loss': [1.799835443496704,\n",
       "  1.7610116004943848,\n",
       "  1.674082636833191,\n",
       "  1.5948578119277954,\n",
       "  1.5519315004348755,\n",
       "  1.520258903503418,\n",
       "  1.5030460357666016,\n",
       "  1.4608428478240967,\n",
       "  1.4500340223312378,\n",
       "  1.4518234729766846,\n",
       "  1.4222009181976318,\n",
       "  1.4270235300064087,\n",
       "  1.4224190711975098,\n",
       "  1.4436089992523193,\n",
       "  1.4264065027236938,\n",
       "  1.6229867935180664,\n",
       "  1.552390217781067,\n",
       "  1.5659531354904175,\n",
       "  1.6415114402770996,\n",
       "  1.6857563257217407,\n",
       "  1.7735538482666016,\n",
       "  1.8240571022033691,\n",
       "  1.9711283445358276,\n",
       "  2.1524128913879395,\n",
       "  2.051297426223755,\n",
       "  2.1983842849731445,\n",
       "  2.1957335472106934,\n",
       "  2.3099002838134766,\n",
       "  2.4653241634368896,\n",
       "  2.477069854736328]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history #Return Dictionary of history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy   : 0.8558917045593262\n",
      "Training Loss       : 0.36520451307296753\n",
      "Validation Accuracy : 0.470752090215683\n",
      "Validation Loss     : 2.477069854736328\n"
     ]
    }
   ],
   "source": [
    "training_accuracy      = history.history['accuracy'][-1]\n",
    "training_loss          = history.history['loss'][-1]\n",
    "validation_accuracy    = history.history['val_accuracy'][-1]\n",
    "validation_loss       = history.history['val_loss'][-1]\n",
    "print(\"Training Accuracy   :\", training_accuracy )\n",
    "print(\"Training Loss       :\", training_loss)\n",
    "print(\"Validation Accuracy :\", validation_accuracy)\n",
    "print(\"Validation Loss     :\", validation_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Model Deployment\n",
    "\n",
    "Save the model to deploy it into a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('emotion_classification_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10: Monitoring and Maintenance\n",
    "\n",
    "Monitoring can be done in production by setting up logging and tracking metrics like accuracy, latency, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "The predicted emotion is: happy\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('emotion_classification_model.h5')\n",
    "\n",
    "# List of emotions corresponding to class indices\n",
    "emotions = ['angry','disgust','fear','happy','neutral','sad','surprise']  # Update this list as needed\n",
    "\n",
    "# Function to process the image and make a prediction\n",
    "def process_image(image_path):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n",
    "    img = cv2.resize(img, (48, 48))  # Resize to match model input\n",
    "    img = img.reshape(1, 48, 48, 1)  # Reshape for the model\n",
    "    img = img.astype('float32') / 255  # Normalize the pixel values\n",
    "    return img\n",
    "\n",
    "# Specify the image path directly\n",
    "my_image = 'sample.PNG'  # Update this path to your image\n",
    "\n",
    "# Proccess the image\n",
    "img = process_image(my_image)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.predict(img)\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "# Get the emotion from the list based on the predicted class index\n",
    "emotion = emotions[predicted_class]\n",
    "\n",
    "print(f'The predicted emotion is: {emotion}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mltutzenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
