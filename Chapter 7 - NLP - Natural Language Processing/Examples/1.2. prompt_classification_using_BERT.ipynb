{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fine-Tuning BERT or DistilBERT for Prompt Classification\n",
    "\n",
    "To use BERT or DistilBERT for prompt parsing, you can fine-tune the model on a small dataset of prompt examples labeled with specific actions (e.g., summarize_single_doc, summarize_multiple_docs, list_key_findings). This enables the model to classify user prompts accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to Fine-Tune BERT or DistilBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, DistilBertTokenizerFast\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "data = {\n",
    "    \"prompt\": [\n",
    "        'Give me the summarize of the document entitled \"LIFT: Cloud-based Management System for Research, Production, and Extension Services.\"',\n",
    "        'Give me the summarize of a document entitled \"LIFT: Cloud-based Management System for Research, Production, and Extension Services.\"',\n",
    "        'Summarize the document entitled \"LIFT: Cloud-based Management System for Research, Production, and Extension Services.\"',\n",
    "        'Summarize a document entitled \"LIFT: Cloud-based Management System for Research, Production, and Extension Services.\"',\n",
    "        \n",
    "        'Give me the summarize of all documents about \"NLP, Machine Learning, IoT, Web-based\".',\n",
    "        'Give me the summarize of all documents with the following keywords \"NLP, Machine Learning, IoT, Web-based\".',\n",
    "        'Give me the summarize of all documents using \"NLP, Machine Learning, IoT, Web-based\".',\n",
    "        'Summarize all documents about \"NLP, Machine Learning, IoT, Web-based\".',\n",
    "        'Summarize all documents with the following keywords \"NLP, Machine Learning, IoT, Web-based\".',\n",
    "        'Summarize all documents using \"NLP, Machine Learning, IoT, Web-based\".',\n",
    "        'Give me the summarize of all documents at year 2024',\n",
    "        'Summarize all documents at year 2024',\n",
    "        \n",
    "        'List all the documents about \"NLP, Machine Learning, IoT, Web-based\".',\n",
    "        'List all the documents with the following keywords \"NLP, Machine Learning, IoT, Web-based\".',\n",
    "        'List all the documents using \"NLP, Machine Learning, IoT, Web-based\".',\n",
    "        'List all the documents at year 2024',\n",
    "        'List all documents about \"NLP, Machine Learning, IoT, Web-based\".',\n",
    "        'List all documents with the following keywords \"NLP, Machine Learning, IoT, Web-based\".',\n",
    "        'List all documents using \"NLP, Machine Learning, IoT, Web-based\".',\n",
    "        'List all documents at year 2024'\n",
    "    ],\n",
    "    \"label\": [0,0,0,0,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2] \n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 19043.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define labels and tokenize prompts\n",
    "labels = [\"summarize_single_doc\", \"summarize_multiple_doc\", \"list_all_doc\"]\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    return tokenizer(examples[\"prompt\"], truncation=True, padding=\"max_length\", max_length=50)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_data, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the model for classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using DistilBERT for this type of task, as it's a lightweight version of BERT. If youâ€™re already using it, consider a smaller model, like albert-base-v2 or tiny-bert, which are even more memory-efficient.\n",
    "\n",
    "Replace distilbert-base-uncased with a smaller model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.sa_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels=len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\nlp\\nlpenv\\lib\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./BERT/results\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:01<00:00,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1.3834, 'train_samples_per_second': 43.37, 'train_steps_per_second': 6.506, 'train_loss': 1.0938706927829318, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9, training_loss=1.0938706927829318, metrics={'train_runtime': 1.3834, 'train_samples_per_second': 43.37, 'train_steps_per_second': 6.506, 'total_flos': 31131702000.0, 'train_loss': 1.0938706927829318, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_distilbert\\\\tokenizer_config.json',\n",
       " './fine_tuned_distilbert\\\\special_tokens_map.json',\n",
       " './fine_tuned_distilbert\\\\vocab.txt',\n",
       " './fine_tuned_distilbert\\\\added_tokens.json',\n",
       " './fine_tuned_distilbert\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./BERT/fine_tuned_distilbert\")\n",
    "tokenizer.save_pretrained(\"./BERT/fine_tuned_distilbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Fine-Tuned Model for Prompt Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./BERT/fine_tuned_distilbert\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"./BERT/fine_tuned_distilbert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define label mapping\n",
    "label_map = {0: \"summarize_single_doc\", 1: \"summarize_multiple_doc\", 2: \"list_all_doc\"}\n",
    "\n",
    "def classify_prompt(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=50)\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return label_map[prediction]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: list_all_doc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "prompt = \"Give me the list of all documents.\"\n",
    "classification = classify_prompt(prompt)\n",
    "print(f\"Classification: {classification}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
